{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model conversion from JAX to TensorFlow Light"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8J1g5vBT5aj"
      },
      "source": [
        "#### References\n",
        "\n",
        "* https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/README.md\n",
        "* https://github.com/google-research/vision_transformer/blob/main/vit_jax.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piv05HW04aUW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tLVMT01KScv5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-nightly 2.11.0.dev20220731 requires flatbuffers>=2.0, but you have flatbuffers 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q absl-py>=0.12.0 chex>=0.0.7 clu>=0.0.3 einops>=0.3.0\n",
        "!pip install -q flax==0.3.3 ml-collections==0.1.0 tf-nightly\n",
        "!pip install -q numpy>=1.19.5 pandas>=1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'vision_transformer'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 50 (delta 5), reused 22 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), 1.65 MiB | 4.79 MiB/s, done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# Clone repository and pull latest changes.\n",
        "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n",
        "!cd vision_transformer && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwBrIdAE4ciM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
            "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (2.9.1)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.22.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.46.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.19.4)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (14.0.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.6.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (62.3.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.34.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.22.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.1.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.6.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (5.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.2.0)\n",
            "Installing collected packages: flatbuffers, tensorflow-hub, tensorflow_text\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-nightly 2.11.0.dev20220731 requires flatbuffers>=2.0, but you have flatbuffers 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 tensorflow-hub-0.12.0 tensorflow_text-2.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -m pip install tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aWuEnEshSdzt"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.8/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2021110220raw_logging_internal21internal_log_functionB5cxx11E",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/ViT/conversion.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f75736d616e766974222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/ViT/conversion.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m./vision_transformer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mpath:\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f75736d616e766974222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/ViT/conversion.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m     sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m./vision_transformer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f75736d616e766974222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/ViT/conversion.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f75736d616e766974222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/ViT/conversion.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m checkpoint\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f75736d616e766974222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/ViT/conversion.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfigs\u001b[39;00m \u001b[39mimport\u001b[39;00m common \u001b[39mas\u001b[39;00m common_config\n",
            "File \u001b[0;32m/ViT/./vision_transformer/vit_jax/models.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2022 Google LLC.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m models_lit\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m models_mixer\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m models_vit\n",
            "File \u001b[0;32m/ViT/./vision_transformer/vit_jax/models_lit.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m checkpoint\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m models_vit\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvit_jax\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflaxformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marchitectures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbert\u001b[39;00m \u001b[39mimport\u001b[39;00m bert\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflaxformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marchitectures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbert\u001b[39;00m \u001b[39mimport\u001b[39;00m configs\n",
            "File \u001b[0;32m/ViT/./vision_transformer/vit_jax/preprocess.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tokenizer\u001b[39m(tokenizer_name):\n\u001b[1;32m     24\u001b[0m   \u001b[39m\"\"\"Returns a tokenizer specified by name (\"bert\" or \"sentencpiece\").\"\"\"\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_text/__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall_util\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_undocumented\n\u001b[1;32m     19\u001b[0m \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpybinds\u001b[39;00m \u001b[39mimport\u001b[39;00m tflite_registrar\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.8/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2021110220raw_logging_internal21internal_log_functionB5cxx11E"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"./vision_transformer\" not in sys.path:\n",
        "    sys.path.append(\"./vision_transformer\")\n",
        "\n",
        "from vit_jax import models\n",
        "from vit_jax import checkpoint\n",
        "from vit_jax.configs import common as common_config\n",
        "from vit_jax.configs import models as models_config\n",
        "\n",
        "from jax.experimental import jax2tf\n",
        "import tensorflow as tf\n",
        "import flax\n",
        "import jax\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFV5xTgW28ys"
      },
      "outputs": [],
      "source": [
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"FLAX version: {flax.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt5uGYJH3LXM"
      },
      "source": [
        "## Classification / Feature Extractor model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKh0k1M7SgSN"
      },
      "outputs": [],
      "source": [
        "#@title Choose a model type\n",
        "VIT_MODELS = \"B_8-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224\" #@param [\"L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224\", \"B_16-i21k-300ep-lr_0.001-aug_medium2-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224\", \"R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224\", \"R26_S_32-i21k-300ep-lr_0.001-aug_light0-wd_0.03-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.03-res_224\", \"R26_S_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224\", \"S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224\", \"B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224\", \"B_8-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224\"]\n",
        "#@markdown The models were selected based on the criteria shown here in [this notebook](https://github.com/sayakpaul/ViT-jax2tf/blob/main/model-selector.ipynb).\n",
        "\n",
        "print(f\"Model type selected: ViT-{VIT_MODELS.split('-')[0]}\")\n",
        "\n",
        "ROOT_GCS_PATH = \"gs://vit_models/augreg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjlhNR62-JJJ"
      },
      "outputs": [],
      "source": [
        "classification_model = True\n",
        "\n",
        "if classification_model:\n",
        "    num_classes = 1000\n",
        "    print(\"Will be converting a classification model.\")\n",
        "else:\n",
        "    num_classes = None\n",
        "    print(\"Will be converting a feature extraction model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mclDoMqbShzV"
      },
      "outputs": [],
      "source": [
        "# Instantiate model class and load the corresponding checkpoints.\n",
        "config = common_config.get_config()\n",
        "config.model = models_config.AUGREG_CONFIGS[f\"{VIT_MODELS.split('-')[0]}\"]\n",
        "\n",
        "model = models.VisionTransformer(num_classes=num_classes, **config.model)\n",
        "\n",
        "path = f\"{ROOT_GCS_PATH}/{VIT_MODELS}.npz\"\n",
        "params = checkpoint.load(path)\n",
        "\n",
        "if not num_classes:\n",
        "    _ = params.pop(\"head\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqbuBCFw9vyg"
      },
      "source": [
        "## Conversion\n",
        "\n",
        "Code has been reused from the official examples [here](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/README.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT2GLwXg95tE"
      },
      "source": [
        "### Step 1: Get a prediction function out of the JAX model & convert it to a native TF function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2e1h-F2SmDB"
      },
      "outputs": [],
      "source": [
        "predict_fn = lambda params, inputs: model.apply(\n",
        "    dict(params=params), inputs, train=False\n",
        ")\n",
        "\n",
        "with_gradient = False if num_classes else True\n",
        "tf_fn = jax2tf.convert(\n",
        "    predict_fn,\n",
        "    with_gradient=with_gradient,\n",
        "    polymorphic_shapes=[None, \"b, 224, 224, 3\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRGAjnKBRGgU"
      },
      "source": [
        "We set `polymorphic_shapes` to allow the converted model operate with arbitrary batch sizes. Know more about the shape polymorphism in JAX from [here](https://github.com/google/jax/tree/main/jax/experimental/jax2tf#shape-polymorphic-conversion)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKE1msyx-3ge"
      },
      "source": [
        "### Step 2: Set the trainability of the individual param groups and construct TF graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RNlRp9pTHgF"
      },
      "outputs": [],
      "source": [
        "param_vars = tf.nest.map_structure(\n",
        "    lambda param: tf.Variable(param, trainable=with_gradient), params\n",
        ")\n",
        "tf_graph = tf.function(\n",
        "    lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fDRubHD_Sf3"
      },
      "source": [
        "### Step 3: Serialize as a SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1QJQwDEyTs2V"
      },
      "outputs": [],
      "source": [
        "#@title SavedModel wrapper class utility from [here](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/saved_model_lib.py#L128)\n",
        "class _ReusableSavedModelWrapper(tf.train.Checkpoint):\n",
        "  \"\"\"Wraps a function and its parameters for saving to a SavedModel.\n",
        "  Implements the interface described at\n",
        "  https://www.tensorflow.org/hub/reusable_saved_models.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, tf_graph, param_vars):\n",
        "    \"\"\"Args:\n",
        "      tf_graph: a tf.function taking one argument (the inputs), which can be\n",
        "         be tuples/lists/dictionaries of np.ndarray or tensors. The function\n",
        "         may have references to the tf.Variables in `param_vars`.\n",
        "      param_vars: the parameters, as tuples/lists/dictionaries of tf.Variable,\n",
        "         to be saved as the variables of the SavedModel.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models\n",
        "    self.variables = tf.nest.flatten(param_vars)\n",
        "    self.trainable_variables = [v for v in self.variables if v.trainable]\n",
        "    # If you intend to prescribe regularization terms for users of the model,\n",
        "    # add them as @tf.functions with no inputs to this list. Else drop this.\n",
        "    self.regularization_losses = []\n",
        "    self.__call__ = tf_graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr2Vf9Ql_lca"
      },
      "outputs": [],
      "source": [
        "input_signatures = [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32)]\n",
        "model_dir = VIT_MODELS if num_classes else f\"{VIT_MODELS}_fe\"\n",
        "signatures = {}\n",
        "saved_model_options = None\n",
        "\n",
        "print(f\"Saving model to {model_dir} directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMn9fJxuTKON"
      },
      "outputs": [],
      "source": [
        "signatures[\n",
        "    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
        "] = tf_graph.get_concrete_function(input_signatures[0])\n",
        "\n",
        "wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars)\n",
        "if with_gradient:\n",
        "    if not saved_model_options:\n",
        "        saved_model_options = tf.saved_model.SaveOptions(\n",
        "            experimental_custom_gradients=True\n",
        "        )\n",
        "    else:\n",
        "        saved_model_options.experimental_custom_gradients = True\n",
        "tf.saved_model.save(\n",
        "    wrapper, model_dir, signatures=signatures, options=saved_model_options\n",
        ")\n",
        "\n",
        "# Note that directly saving the `wrapper` to a GCS location is\n",
        "# also supported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJr-uVs_vz-"
      },
      "source": [
        "## Functional test (credits: [Willi Gierke](https://ch.linkedin.com/in/willi-gierke))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA2G4HzvC5_l"
      },
      "source": [
        "### Image preprocessing utilities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyvjkBAE5iFL"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    image = np.array(image)\n",
        "    image_resized = tf.image.resize(image, (224, 224))\n",
        "    image_resized = tf.cast(image_resized, tf.float32)\n",
        "    image_resized = (image_resized - 127.5) / 127.5\n",
        "    return tf.expand_dims(image_resized, 0).numpy()\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    image = preprocess_image(image)\n",
        "    return image\n",
        "\n",
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt -O ilsvrc2012_wordnet_lemmas.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd-YH-hqAIQ9"
      },
      "source": [
        "### Load image and ImageNet-1k class mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vDQd6MEAEp_"
      },
      "outputs": [],
      "source": [
        "with open(\"ilsvrc2012_wordnet_lemmas.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
        "\n",
        "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
        "image = load_image_from_url(img_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-LxOYBANnv"
      },
      "source": [
        "### Inference\n",
        "\n",
        "This is only application for the classification models. For fine-tuning/feature extraction, please follow [this notebook](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/fine_tune.ipynb) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99DTGYB25o5d"
      },
      "outputs": [],
      "source": [
        "# Load the converted SavedModel and check whether it finds the elephant.\n",
        "restored_model = tf.saved_model.load(model_dir)\n",
        "predictions = restored_model.signatures[\"serving_default\"](tf.constant(image))\n",
        "logits = predictions[\"output_0\"][0]\n",
        "predicted_label = imagenet_int_to_str[int(np.argmax(logits))]\n",
        "expected_label = \"Indian_elephant, Elephas_maximus\"\n",
        "assert (\n",
        "    predicted_label == expected_label\n",
        "), f\"Expected {expected_label} but was {predicted_label}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itfezjKjAXA6"
      },
      "source": [
        "## Inference with TensorFlow Hub \n",
        "\n",
        "Run the following code snippet. You can also follow [this notebook](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/classification.ipynb). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR8lV2377Ad3"
      },
      "source": [
        "```python\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "classification_model = tf.keras.Sequential([hub.KerasLayer(model_dir)])\n",
        "predictions = classification_model.predict(image)\n",
        "predicted_label = imagenet_int_to_str[int(np.argmax(predictions))]\n",
        "predicted_label\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "conversion",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
